name: UCSP Algorithm - Weekly Data Scrape

# Trigger del workflow
on:
  # Ejecutar cada lunes a las 1 PM UTC (8 AM Perú)
  schedule:
    - cron: '0 13 * * 1'

  # Permitir ejecución manual desde GitHub Actions UI
  workflow_dispatch:

jobs:
  scrape-and-commit:
    name: Scrape UCSP data from all sources
    runs-on: ubuntu-latest

    steps:
      # 1. Checkout del código
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0

      # 2. Setup Python 3.10
      - name: Setup Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'
          cache-dependency-path: 'scrapers/requirements.txt'

      # 3. Setup Node.js 18
      - name: Setup Node.js 18
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
          cache-dependency-path: 'scrapers/package-lock.json'

      # 4. Instalar dependencias Python
      - name: Install Python dependencies
        working-directory: ./scrapers
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      # 5. Instalar dependencias Node
      - name: Install Node.js dependencies
        working-directory: ./scrapers
        run: |
          npm ci

      # 6. Crear directorios de datos si no existen
      - name: Create data directories
        run: |
          mkdir -p data/trends
          mkdir -p data/tiktok
          mkdir -p data/meta
          mkdir -p public/data/trends
          mkdir -p public/data/tiktok
          mkdir -p public/data/meta

      # 7. Ejecutar scraper de Google Trends con Apify
      - name: Run Google Trends Scraper (Apify)
        working-directory: ./scrapers
        run: |
          echo "Scraping Google Trends via Apify for UCSP..."
          node google_trends_apify.js --client=ucsp
        env:
          APIFY_TOKEN: ${{ secrets.APIFY_TOKEN }}
        continue-on-error: true

      # 8. Ejecutar scraper de TikTok
      - name: Run TikTok Scraper (UCSP)
        working-directory: ./scrapers
        run: |
          echo "Scraping TikTok Creative Center for education trends..."
          node tiktok_scraper.js
        continue-on-error: true

      # 9. Ejecutar scraper de Meta
      - name: Run Meta Public Trends Scraper (UCSP)
        working-directory: ./scrapers
        run: |
          echo "Scraping Meta public trends for UCSP..."
          node meta_scraper.js
        continue-on-error: true

      # 10. Verificar archivos generados
      - name: Check generated files
        run: |
          echo "Verificando archivos generados..."
          echo "=== Google Trends ==="
          ls -lah data/trends/ || echo "No trends data"
          echo "=== TikTok ==="
          ls -lah data/tiktok/ || echo "No tiktok data"
          echo "=== Meta ==="
          ls -lah data/meta/ || echo "No meta data"

      # 11. Commit y push de los datos
      - name: Commit and push data
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "UCSP Algorithm Bot"

          # Agregar archivos de datos
          git add data/ public/data/

          # Verificar si hay cambios
          if git diff --staged --quiet; then
            echo "No hay cambios en los datos"
          else
            # Commit con timestamp
            TIMESTAMP=$(date +'%Y-%m-%d %H:%M UTC')
            git commit -m "Update UCSP social listening data - $TIMESTAMP"

            # Push
            git push

            echo "Datos UCSP actualizados y pusheados"
          fi

      # 12. Notificación de éxito
      - name: Success notification
        if: success()
        run: |
          echo "UCSP Algorithm scraping completado exitosamente"
          echo "Próxima ejecución: Próximo lunes a las 8 AM (Perú)"

      # 13. Notificación de error
      - name: Error notification
        if: failure()
        run: |
          echo "UCSP Algorithm scraping falló"
          echo "Revisa los logs arriba para más detalles"
